# 🔐 PARAG: Proactive Answering Framework Integrating LLMs with Retrieval-Augmented Generation

PARAG is a full-stack application designed to help organizations intelligently query their internal policy documents using Retrieval-Augmented Generation (RAG) powered by Large Language Models (LLMs).

This repository includes:

- 🧠 A **backend** built with FastAPI for processing PDF documents, indexing them using vector embeddings, and responding to intelligent queries.
- 🌐 A **frontend** built with modern JavaScript frameworks (e.g., React + Vite + Tailwind) to provide an intuitive user interface.

---

## 🗂️ Repository Structure

```
PARAG/
|── Dataset/
|   ├── Comprehensive..csv # Dataset used to evaluate the framework 
├── PARAG-FRONTEND/        # Frontend application
│   ├── public/            # Static assets
│   ├── README.md          # ReadMe file for frontend
│   ├── src/               # Source code
│   ├── index.html         # Root HTML
│   ├── tailwind.config.js # Tailwind CSS configuration
│   └── package.json       # Frontend metadata and dependencies
├── backend/               # FastAPI backend for PDF ingestion and RAG query
│   ├── app/               # Core backend app
│   └── README.md          # ReadMe file for backend
└── README.md              # Root documentation (this file)
```

---

## ⚙️ Prerequisites

- **Python 3.10+** (for the backend)
- **Node.js (LTS)** and **npm/yarn** (for the frontend)
- An OpenAI API key

---

## 🛠️ Backend Setup

1. Navigate to the backend folder:

```bash
cd backend
```

2. Create a virtual environment and activate it:

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:

```bash
pip install -r requirements.txt
```

4. Create a `.env` file in the backend root and add your OpenAI key:

```
OPENAI_API_KEY=your-api-key-here
```

5. Run the backend:

```bash
uvicorn app.main:app --reload
```

---

## 🌐 Frontend Setup

1. Navigate to the frontend directory:

```bash
cd PARAG-FRONTEND
```

2. Install dependencies:

```bash
npm install
```

3. Start the development server:

```bash
npm run dev
```

This will typically serve the app at: `http://localhost:5173`

---

## 📡 Connecting Frontend to Backend

Ensure the backend is running on `http://localhost:8000`. If the frontend fetch requests need configuration, update the endpoint in your fetch logic or environment config.

---

## 🧪 Example Use Cases

- Go to `http://localhost:5173` in your browser
- Upload a multi-page organizational policy PDF
- Ask: “What are the steps for system isolation in case of a security breach?”
- Receive accurate responses generated by RAG and grounded in your content

---

## 🤝 Contributing

We welcome contributions and improvements!  
Feel free to open issues or submit pull requests.


---

## 📜 License & Citation

This project is licensed under the **Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)** license.

You are free to:

- **Share** — copy and redistribute the material in any medium or format  
- **Adapt** — remix, transform, and build upon the material  

Under the following terms:

- **Attribution** — You must give appropriate credit, provide a link to the license, and indicate if changes were made.  
- **NonCommercial** — You may not use the material for commercial purposes.

🔗 License Details: [https://creativecommons.org/licenses/by-nc/4.0/](https://creativecommons.org/licenses/by-nc/4.0/)

If you use or build upon this work in your research or application, please **cite the following publication**:

> **Bikash Saha, Nanda Rani, Joheen Chakraborty, Divyanshu Singh, Soumyo V. Chakraborty, and Sandeep K. Shukla**  
> *PARAG: Proactive Answering Framework Integrating LLMs with Retrieval-Augmented Generation.*  
> In: Praça I., Bernardi S., Inácio P.R. (eds) **Cybersecurity. EICC 2025.** Communications in Computer and Information Science, vol 2500. Springer, Cham.  
> [https://doi.org/10.1007/978-3-031-94855-8_2](https://doi.org/10.1007/978-3-031-94855-8_2)